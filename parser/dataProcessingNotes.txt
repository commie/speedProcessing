// duplicate location extraction on Aug 30 ///////////////////////////////

...
	7:28:15 PM Done reading file.
	7:28:15 PM A total of 80163972 records parsed, 80163972 valid tweets found.
	7:28:15 PM Last tweet # is 947618361709678592

	Duplicate locations are now printed to stderr ...
	Seen 9,207,960 locations in the original file, processed 9,207,960 locations when looking for duplicates.
	... done.


realized that we're including duplicate tweets in the count of duplicate locations
	re-wrote the script to exclude duplicate tweets before any location processing is done


ran duplicate location extraction again

	4:01:49 PM Done reading file.
	4:01:49 PM A total of 80,163,972 records parsed, 80,163,972 valid tweets found.
	4:01:49 PM Last tweet # is 947618361709678592

	Duplicate locations are now printed to stderr ...
	Seen 8,732,638 locations in the original file, processed 8,732,638 locations when looking for duplicates.
	... done.

	
	duplicateLocations.02.out
	
	8,732,638	total locations seen (after excluding tweets with duplicate IDs)
	7,583,276	duplicate locations seen
		
	1,149,362	unique
		13.2%	of non-place locations seen
	 	 1.4%	of all geo tweets processed


ran a script to separate unique and duplicate locations into separate files

	distributedReader.2.1.twitterCrawler01.2017.12.merged.dupLoc.out
	distributedReader.2.1.twitterCrawler01.2017.12.merged.uniqueLoc.out
	distributedReader.2.1.twitterCrawler01.2017.12.merged.userLocStat.out	// user id, user name, unique count, duplicate count, total, unique percent


ran movement extraction for unique and duplicate location files

	distributedReader.2.1.twitterCrawler01.2017.12.merged.uniqueLoc.movement.out
	distributedReader.2.1.twitterCrawler01.2017.12.merged.dupLoc.movement.out
	
	file have 1,212,780 and 7,995,180 tweets, respectively
	this is equal to the 9,207,960 locations in the original file!

Heatmaps for duplicate and unique locations
	DUR-DIST_grid_dup.png , svg
	DUR-DIST_grid_unique.png ,svg

Heatmap of duplicate - unique counts difference usinfg the grid generatd based on the total movement record file
	DUR-DIST_dup_un_diff.png, svg


user heterogeneity for all data

break into subsets
	1) by visual density (clusters)
	2) by transport modality (logical subset)

extract top hashtag for clusters

TODO

Filter top offenders from central blob and draw the remainder
production grade figures (axis captions, grid line captions)

leterature survey (social media + speed of movements + data cleaning + similar projects)


*TODO to the next article (hadoop)

extract Keywords, for each cell in heatmap and apply clustering

Time series of heatmaps (weakly, monthly, annualy)
